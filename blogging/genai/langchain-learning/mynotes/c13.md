Here’s a tighter, clearer, and friendlier version of your guide—same examples retained verbatim, plus callouts, tips, and gotchas to make it easy to follow.

---

# Retrieval-Augmented Generation (RAG) — Practical Guide

RAG = **Retrieval Augmented Generation**.
In plain terms: you ask an LLM a question **and** give it just the **relevant** facts from your data right when you ask.

> **Why care?**
>
> * Your model might be trained on **outdated** public data.
> * Your question might require **private/company** knowledge the model never saw.
>   RAG bridges both gaps by retrieving fresh, relevant snippets and feeding them to the LLM with your prompt.

---

## When to Use RAG

### Use-case 1 — Latest info

* The LLM is trained on older data, but your question needs **current** facts.
* RAG fetches up-to-date context for the model to use in its answer.

### Use-case 2 — Private/company data

* You want answers grounded in **internal** docs, FAQs, tickets, PDFs, etc.
* RAG selects only the **relevant** chunks at query time.

> **Core idea:** send **Prompt + Relevant_Data** to the LLM—**not** gigabytes of everything.

---

## How RAG Works (High Level)

1. **Chunk** your documents (text, PDF, DOCX, etc.).
2. **Embed** the chunks and **store** them in a vector store.
3. On each question:

   * Compute an **embedding** for the question.
   * **Retrieve** the most similar chunks.
   * **Stuff** (inject) those chunks into the LLM prompt and generate an answer.

> **Tip**: Start with chunk sizes of 700–1200 characters and ~10–20% overlap. Tune later based on answer quality.

> **Gotcha**: Retrieval ≠ reasoning. If the retriever pulls irrelevant chunks, the LLM will reason over the wrong facts. Always validate retrieval quality.

---

## Minimal RAG with LangChain (kept as you wrote it)

All this can be done with the help of LangChain with easy steps!

**Sample Code**

```python
import os
from langchain_openai import OpenAIEmbeddings,ChatOpenAI
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain


OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
embeddings=OpenAIEmbeddings(api_key=OPENAI_API_KEY)
llm=ChatOpenAI(model="gpt-4o", api_key=OPENAI_API_KEY)

document = TextLoader("product-data.txt").load()
text_splitter= RecursiveCharacterTextSplitter(chunk_size=1000,
                                              chunk_overlap=200)
chunks=text_splitter.split_documents(document)
vector_store=Chroma.from_documents(chunks,embeddings)
retriever = vector_store.as_retriever()

prompt_template = ChatPromptTemplate.from_messages(
[
    ("system","""You are an assistant for answering questions.
    Use the provided context to respond.If the answer 
    isn't clear, acknowledge that you don't know. 
    Limit your response to three concise sentences.
    {context}
    
    """),
    ("human", "{input}")
]
)

qa_chain = create_stuff_documents_chain(llm,prompt_template)
rag_chain = create_retrieval_chain(retriever,qa_chain)

print("Chat with Document")
question=input("Your Question")

if question:
    response = rag_chain.invoke({"input":question})
    print(response['answer'])
```

### What to notice

* `{context}` is the placeholder the retriever fills with the relevant chunks.
* `create_stuff_documents_chain` handles **stuffing** the retrieved context into your prompt.
* `create_retrieval_chain` pairs a **retriever** with the **QA chain** to handle the end-to-end flow.

> **Quality tip**: If answers feel vague, try increasing the number of retrieved documents (e.g., `retriever.search_kwargs={"k": 4}`) and add a reranker (e.g., MMR or a cross-encoder) later.

**Try these queries** (assuming `product-data.txt` has product reviews/FAQs):

1. What is the status of my order #98765?
2. Can you provide details about the features of the XYZ smartphone?
3. What is your return policy for electronics?
4. I received a damaged item in my order. How can I get a replacement?
5. Can you recommend a good smartphone for gaming under $500?
6. How do I apply for a job at your company?

---

## Using Local LLMs & Local Embeddings

> **Note**: Corporate firewalls or network policies can block hosted APIs. Local stacks are a great fallback.

You used **OllamaEmbeddings** and the **gemma:2b** model locally. Good call.

```python
from langchain.embeddings.ollama import OllamaEmbeddings
embeddings = OllamaEmbeddings(model="mxbai-embed-large:latest")
```

This worked!

**Working solution (kept as you wrote it)**

```python
import os
from langchain_community.chat_models import ChatOllama
from langchain_community.document_loaders import TextLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.embeddings.ollama import OllamaEmbeddings

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
embeddings = OllamaEmbeddings(model="mxbai-embed-large:latest")
llm =ChatOllama(model="gemma:2b")

document = TextLoader("product-data.txt").load()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,
                                               chunk_overlap=200)
chunks = text_splitter.split_documents(document)
vector_store = Chroma.from_documents(chunks, embeddings)
retriever = vector_store.as_retriever()

prompt_template = ChatPromptTemplate.from_messages(
    [
        ("system", """You are an assistant for answering questions.
    Use the provided context to respond.If the answer 
    isn't clear, acknowledge that you don't know. 
    Limit your response to three concise sentences.
    {context}

    """),
        ("human", "{input}")
    ]
)

qa_chain = create_stuff_documents_chain(llm, prompt_template)
rag_chain = create_retrieval_chain(retriever, qa_chain)

print("Chat with Document")
question = input("Your Question")

if question:
    response = rag_chain.invoke({"input": question})
    print(response['answer'])
```

> **Local tip**: For speed on CPU, prefer smaller GGUF models; for quality, consider 7B+ with GPU acceleration.

---

## RAG + Chat History + Streamlit (kept as you wrote it)

Combine persistent chat history with retrieval for follow-ups that rely on prior turns.

```python
import os
from langchain_community.chat_models import ChatOllama
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.chains import create_retrieval_chain, create_history_aware_retriever
from langchain.chains.combine_documents import create_stuff_documents_chain
import streamlit as st
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.embeddings.ollama import OllamaEmbeddings

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
llm =ChatOllama(model="gemma:2b")

loader=TextLoader("product-data.txt")
documents = loader.load()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)

chunks = text_splitter.split_documents(documents)

embeddings = OllamaEmbeddings(model="mxbai-embed-large:latest")
vector_store=Chroma.from_documents(chunks,embeddings)
retriever = vector_store.as_retriever()

prompt_template = ChatPromptTemplate.from_messages(
    [
        ("system", """You are an assistant for answering questions.
    Use the provided context to respond.If the answer 
    isn't clear, acknowledge that you don't know. 
    Limit your response to three concise sentences.
    {context}

    """),
    MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}")
    ]
)

history_aware_retriever = create_history_aware_retriever(llm, retriever,prompt_template)
qa_chain=create_stuff_documents_chain(llm,prompt_template)
rag_chain=create_retrieval_chain(history_aware_retriever,qa_chain)

history_for_chain=StreamlitChatMessageHistory()

chain_with_history=RunnableWithMessageHistory(
    rag_chain,
    lambda session_id:history_for_chain,
    input_messages_key="input",
    history_messages_key="chat_history"
)

st.write("Chat with Document")
question= st.text_input("Your Question")

if question:
    response = chain_with_history.invoke({"input": question},{"configurable":{"session_id":"abc123"}})
    st.write(response['answer'])
```

### What changed vs. basic RAG?

* You swapped `retriever` for a **history-aware** retriever so follow-up questions use prior turns for better retrieval.

> **UX tip**: Add a chat sidebar showing retrieved sources. Users love traceability.

---

## “Legal Question BOT” (same steps, different file)

* Reuse the Streamlit + history setup; swap in your legal dataset.
* Data & prompts you referenced:

  * `Legal_Document_Analysis_Data.txt`
  * `legal_questions.txt`

> **Caution**: Add disclaimers. LLMs aren’t a substitute for legal counsel.

---

## Other File Formats for RAG

* **PDFs**: install `pypdf`; use `PyPDFLoader`.
* **Word (.docx)**: extract with `docx2txt` or equivalent; use `Docx2txtLoader`.

**Sample Code for PDF loading + RAG (kept as you wrote it)**

```python
import os

from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.embeddings.ollama import OllamaEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain_chroma import Chroma
from langchain_community.chat_models import ChatOllama
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
embeddings = OllamaEmbeddings(model="mxbai-embed-large:latest")
llm =ChatOllama(model="gemma:2b")

document = PyPDFLoader("academic_research_data.pdf").load()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,
                                               chunk_overlap=200)
chunks = text_splitter.split_documents(document)
vector_store = Chroma.from_documents(chunks, embeddings)
retriever = vector_store.as_retriever()

prompt_template = ChatPromptTemplate.from_messages(
    [
        ("system", """You are an assistant for answering questions.
    Use the provided context to respond.If the answer 
    isn't clear, acknowledge that you don't know. 
    Limit your response to three concise sentences.
    {context}

    """),
        ("human", "{input}")
    ]
)

qa_chain = create_stuff_documents_chain(llm, prompt_template)
rag_chain = create_retrieval_chain(retriever, qa_chain)

print("Chat with Document")
question = input("Your Question")

if question:
    response = rag_chain.invoke({"input": question})
    print(response['answer'])
```

---

