# ðŸª„ **LangChain: Understanding Chains**

LangChain uses a syntax called **LCEL** â€” which stands for **LangChain Expression Language**.
It allows you to **compose complex workflows** by chaining different components together using the `|` (pipe) operator.

---

## ðŸ”— **1. Simple Sequential Chain**

> A **Sequential Chain** connects multiple steps so that the **output of one chain** becomes the **input of the next**.

### ðŸ“˜ Concept

* Think of it like a **pipeline**.
* Each chain can have:

  * its **own prompt**,
  * an **LLM call**,
  * and a **parser or transformation** step.

### ðŸ§© Example Flow

```
Chain 1 | Chain 2
```

* Output from **Chain 1** is **fed as input** to **Chain 2**.

---

## ðŸ” **2. Sequential Chain (with Multiple Inputs/Outputs)**

> When workflows become more complex, LangChain allows **multiple inputs and outputs** across chained components.

### ðŸ§  How It Works

* You can **merge external inputs** into a chain along the way.
* The output from one chain doesnâ€™t have to be the only input to the next â€” you can **add or override** with new data.
* Similarly, a chain can produce **multiple outputs** (e.g., text + structured data).

### ðŸ’¡ Example Scenario

> Suppose Chain1 generates a **travel itinerary**, while Chain2 adds **hotel recommendations** using both the itinerary and userâ€™s budget.
> This setup lets Chain2 receive data from **Chain1 + user input** simultaneously.

---

## âœˆï¸ **3. Example: Simple Sequential Chain in Action**

Hereâ€™s a working example that uses `PromptTemplate` and `ChatOpenAI` to create a travel guide.

```python
import os
from langchain_openai import ChatOpenAI
import streamlit as st
from langchain.prompts import PromptTemplate

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
llm = ChatOpenAI(model="gpt-4o", api_key=OPENAI_API_KEY)

prompt_template = PromptTemplate(
    input_variables=["city", "month", "language", "budget"],
    template="""Welcome to the {city} travel guide!
    If you're visiting in {month}, here's what you can do:
    1. Must-visit attractions.
    2. Local cuisine you must try.
    3. Useful phrases in {language}.
    4. Tips for traveling on a {budget} budget.
    Enjoy your trip!
    """
)

st.title("Travel Guide")

city = st.text_input("Enter the city:")
month = st.text_input("Enter the month of travel")
language = st.text_input("Enter the language:")
budget = st.selectbox("Travel Budget", ["Low", "Medium", "High"])

chain = prompt_template | llm

if city and month and language and budget:
    response = chain.invoke({
        "city": city,
        "month": month,
        "language": language,
        "budget": budget
    })
    st.write(response.content)
```

---

### ðŸ” **Explanation**

* **`chain = prompt_template | llm`**

  * The `|` operator **connects** the prompt template to the language model.
  * The **output of `PromptTemplate`** becomes the **input for `LLM`**.

* **`chain.invoke()`**

  * This method **runs** the chain.
  * It expects a **dictionary** of keyâ€“value pairs matching the templateâ€™s input variables.

---

> ðŸ’¬ **Tip:**
> `chain.invoke()` takes a **dictionary** object â€” similar to a **map** in JavaScript or Java.
> This distinction is important because each variable name must match exactly with those defined in the `PromptTemplate`.

---

### âœ… **Key Takeaways**

* LCEL (`|`) lets you **compose chains easily** in a declarative style.
* A **Simple Sequential Chain** passes outputs directly between steps.
* A **Sequential Chain** supports **multiple inputs and outputs**, allowing richer workflows.
* Always use `chain.invoke()` with a **dictionary** to run your chain.

