# ðŸ§  Mastering Prompt Templates with LangChain

Want to make your prompts dynamic and reusable?
**Prompt Templates** are your best friend! Letâ€™s explore how they work â€” step by step.

---

## ðŸš€ Using Prompt Templates

Take a look at this code example ðŸ‘‡

```python
import os
from langchain_openai import ChatOpenAI
import streamlit as st
from langchain.prompts import PromptTemplate

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
llm = ChatOpenAI(model="gpt-4o", api_key=OPENAI_API_KEY)

prompt_template = PromptTemplate(
    input_variables=["country", "no_of_paras", "language"],
    template="""You are an expert in traditional cuisines.
    You provide information about a specific dish from a specific country.
    Avoid giving information about fictional places. If the country is fictional
    or non-existent answer: I don't know.
    Answer the question: What is the traditional cuisine of {country}?
    Answer in {no_of_paras} short paras in {language}
    """
)

st.title("Cuisine Info")

country = st.text_input("Enter the country:")
no_of_paras = st.number_input("Enter the number of paras", min_value=1, max_value=5)
language = st.text_input("Enter the language:")

if country:
    response = llm.invoke(prompt_template.format(
        country=country,
        no_of_paras=no_of_paras,
        language=language
    ))
    st.write(response.content)
```

ðŸ§© **Whatâ€™s happening here?**
Weâ€™re using the `PromptTemplate` class with two key arguments:

1. **`input_variables`** â€” defines which variables can change.
2. **`template`** â€” the actual text prompt that uses these variables.

---

## ðŸ”‘ Key Concept: `input_variables`

* These are the **placeholders** your prompt will accept.
* You list them as an **array**, e.g. `["country", "no_of_paras", "language"]`.
* The values come from user input â€” for example, through **Streamlit** widgets like `st.text_input` or `st.number_input`.

ðŸ’¡ Think of `input_variables` as the **â€œblanksâ€** in your fill-in-the-blank sentence!

---

## ðŸ”§ Key Concept: `template`

The `template` defines your **prompt structure** â€” the message youâ€™ll send to the LLM.

* Use **curly braces `{}`** to inject variables dynamically (like `{country}` and `{language}`).
* When you call `prompt_template.format()`, those braces get replaced with the actual user inputs.

> ðŸ—£ï¸ **Analogy:** The `PromptTemplate` works just like Pythonâ€™s **f-string** (e.g. `f"Hello {name}"`) â€” but with one major advantage:
> âœ… **Validation!** It ensures all your input variables are correctly defined before the LLM call.

---

## ðŸ§© Understanding f-prompting

If youâ€™re new to **f-strings** in Python, hereâ€™s a quick refresher:

```python
name = "Kaushik"
salutation = f"Hello {name}"
print(salutation)
```

ðŸ“¤ Output:

```
Hello Kaushik
```

This same principle is what `PromptTemplate` builds upon â€” but with more flexibility and safety.

---

## âš™ï¸ Improving the Prompt with Guardrails

Imagine asking:

> â€œWhatâ€™s the traditional cuisine on Mars?â€

Without context, the model might **invent** something! ðŸ›¸ðŸ²

To prevent this, we can **add a rule** in the prompt to reject fictional places.

Just include this line in your `template`:

```
Avoid giving information about fictional places. 
If the country is fictional or non-existent answer: I don't know.
```

ðŸŽ¯ Thatâ€™s prompt engineering in action â€” making your AI **more accurate and reliable**.

---

## ðŸ’ª Exercise: Build Your Own Prompt Template!

Practice time!
Your task: **Create a prompt template** that answers questions about your favorite **book**, **movie**, or **topic**.

### ðŸ§­ Hints

* What should go into `input_variables`?
* How would your `template` look?

---

### âœ… Sample Solution

```python
import os
from langchain_openai import ChatOpenAI
import streamlit as st
from langchain.prompts import PromptTemplate

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
llm = ChatOpenAI(model="gpt-4o", api_key=OPENAI_API_KEY)

prompt_template = PromptTemplate(
    input_variables=["topic", "question"],
    template="""
    You are a helpful assistant with expertise in {topic}. 
    Answer the following question concisely:
    Question: {question}
    """
)

st.title("Q&A Bot")

topic = st.text_input("Specify a topic")
question = st.text_input("Enter a question:")

if topic:
    response = llm.invoke(prompt_template.format(topic=topic, question=question))
    st.write(response.content)
```

---

## ðŸ§¾ Summary: Steps to Use Prompt Templates

Hereâ€™s a quick recap ðŸ‘‡

1. **Initialize the LLM**

   ```python
   llm = ChatOpenAI(model="gpt-4o", api_key=OPENAI_API_KEY)
   ```

2. **Create a Prompt Template**

   ```python
   prompt_template = PromptTemplate(input_variables=[...], template="...")
   ```

3. **Collect Inputs** from the user (e.g., Streamlit, CLI, etc.)

4. **Format the Prompt**

   ```python
   formatted_prompt = prompt_template.format(topic=topic, question=question)
   ```

   > Returns a **string** with user inputs dynamically injected.

5. **Invoke the LLM**

   ```python
   response = llm.invoke(formatted_prompt)
   ```

6. **Display the Output**

   ```python
   st.write(response.content)
   ```

---

âœ¨ **Pro Tip:**
Prompt Templates make your code **cleaner, safer, and more flexible** â€” perfect for building real-world AI apps that rely on structured user input.