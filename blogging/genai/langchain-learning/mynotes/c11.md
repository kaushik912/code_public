# ðŸ§  Understanding ChatPrompts in LangChain

When building conversational apps, we often want our chatbot to behave consistently â€” for example, as an *Agile Coach*, *Tech Expert*, or *Friendly Assistant*.
Thatâ€™s where **ChatPromptTemplate** comes in.

---

## ðŸš€ What is `ChatPromptTemplate`?

`ChatPromptTemplate` allows you to define **roles** (like system, user, or AI) in your prompt so that you can structure how your model responds.

> ðŸ’¡ **Quick Fact:**
> OpenAI supports three key roles:
>
> * **system** â€“ defines the chatbotâ€™s behavior or persona
> * **human** (or user) â€“ represents user input
> * **ai** â€“ represents model output (used implicitly)

---

### ðŸ§© Example: Simple Agile Coach App

```python
import os
from langchain_openai import ChatOpenAI
import streamlit as st
from langchain.prompts import ChatPromptTemplate

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
llm = ChatOpenAI(model="gpt-4o", api_key=OPENAI_API_KEY)

prompt_template = ChatPromptTemplate.from_messages([
    ("system", "You are an Agile Coach. Answer any questions related to the agile process."),
    ("human", "{input}")
])

st.title("Agile Guide")

input = st.text_input("Enter the question:")

chain = prompt_template | llm

if input:
    response = chain.invoke({"input": input})
    st.write(response.content)
```

---

### ðŸ§  Key Points

* `ChatPromptTemplate.from_messages()` expects a list of **tuples**.

  * Each tuple has the format: `(role, message)`
  * Example:

    ```python
    ("system", "You are an Agile Coach...")
    ("human", "{input}")
    ```
  * The `{input}` placeholder dynamically captures user input.

> ðŸ’¬ **Callout:**
> If you ask a *follow-up question* (e.g., â€œSummarize it in 2 pointsâ€),
> the model wonâ€™t know the context â€” because **chat history isnâ€™t tracked yet**.

To fix this, we need to **add chat memory**.

---

## ðŸ§± Implementing Chat History

To make conversations contextual, weâ€™ll use a few key components:

* **`MessagesPlaceholder`** â†’ placeholder for chat history
* **`ChatMessageHistory`** â†’ stores past messages
* **`RunnableWithMessageHistory`** â†’ injects and manages the history dynamically

---

### ðŸ”§ Example: Adding Chat History (Console Version)

```python
import os
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.chat_message_histories.in_memory import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
llm = ChatOpenAI(model="gpt-4o", api_key=OPENAI_API_KEY)

prompt_template = ChatPromptTemplate.from_messages([
    ("system", "You are an Agile Coach. Answer any questions related to the agile process."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
])

chain = prompt_template | llm
history_for_chain = ChatMessageHistory()

chain_with_history = RunnableWithMessageHistory(
    chain,
    lambda session_id: history_for_chain,
    input_messages_key="input",
    history_messages_key="chat_history"
)

print("Agile Guide")

while True:
    question = input("Enter the question: ")
    if question:
        response = chain_with_history.invoke(
            {"input": question},
            {"configurable": {"session_id": "abc123"}}
        )
        print(response.content)
```

---

### ðŸ’¡ Points to Ponder

* The **`MessagesPlaceholder`** tracks ongoing dialogue by linking to `chat_history`.
* The **`RunnableWithMessageHistory`** acts as a *wrapper* around the chain:

  * It automatically injects chat history when invoking the model.
* The **`session_id`** (e.g., `"abc123"`) uniquely identifies each conversation.

> âš ï¸ **Important:**
> The `configurable={"session_id": "..."}` argument is **mandatory**.
> Without it, the chain wonâ€™t know which chat history to use.

---

### ðŸ§© Step-by-Step Summary

| Step | What You Do                               | Why It Matters                  |
| ---- | ----------------------------------------- | ------------------------------- |
| 1    | Use `ChatPromptTemplate.from_messages()`  | Define the prompt and roles     |
| 2    | Add `MessagesPlaceholder("chat_history")` | Store and reuse message history |
| 3    | Initialize `ChatMessageHistory()`         | Keep chat logs in memory        |
| 4    | Wrap with `RunnableWithMessageHistory`    | Connect everything              |
| 5    | Pass `session_id` on `.invoke()`          | Maintain separate conversations |

---

## ðŸ’¬ Streamlit Version (with Chat History)

Letâ€™s bring it to the browser with **Streamlit**.

```python
import os
from langchain_openai import ChatOpenAI
import streamlit as st
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
llm = ChatOpenAI(model="gpt-4o", api_key=OPENAI_API_KEY)

prompt_template = ChatPromptTemplate.from_messages([
    ("system", "You are an Agile Coach. Answer any questions related to the agile process."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
])

chain = prompt_template | llm
history_for_chain = StreamlitChatMessageHistory()

chain_with_history = RunnableWithMessageHistory(
    chain,
    lambda session_id: history_for_chain,
    input_messages_key="input",
    history_messages_key="chat_history"
)

st.title("Agile Guide")

input = st.text_input("Enter the question:")

if input:
    response = chain_with_history.invoke(
        {"input": input},
        {"configurable": {"session_id": "abc123"}}
    )
    st.write(response.content)

st.write("HISTORY")
st.write(history_for_chain)
```

---

### ðŸª„ Whatâ€™s New in the Streamlit Version?

| Feature                           | Description                                                                    |
| --------------------------------- | ------------------------------------------------------------------------------ |
| **`StreamlitChatMessageHistory`** | Replaces `ChatMessageHistory` to persist chat inside Streamlitâ€™s session state |
| **Same mapping logic**            | You still map `input` â†’ user message and `chat_history` â†’ history placeholder  |
| **Optional History Display**      | You can print chat logs using `st.write(history_for_chain)`                    |

---

### ðŸ“‹ Quick Recap

âœ… Use `ChatPromptTemplate.from_messages()` for defining roles
âœ… Add `MessagesPlaceholder("chat_history")` for memory
âœ… Wrap your chain with `RunnableWithMessageHistory`
âœ… Pass a unique `session_id` each time
âœ… In Streamlit, use `StreamlitChatMessageHistory` for built-in persistence

---

### ðŸ’¬ Pro Tip

If you plan to support **multiple users** or **multi-session chatbots**,
use **unique session IDs** (like user IDs or timestamps) to separate histories.
