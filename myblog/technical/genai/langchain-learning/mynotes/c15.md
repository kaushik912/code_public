# ðŸ§  Building an AI Agent with LangChain

## Whatâ€™s an AI Agent?

An **AI Agent** is a smart software component that can **perform tasks autonomously**.
It does this by combining the power of:

* **LLM intelligence** (for reasoning and decision-making)
* **Tools** (to take actions and fetch data)

> ðŸ’¡ Think of an AI Agent as a digital assistant that *not only thinks* but also *acts* using the tools you give it.

---

## ðŸ” How an AI Agent Works â€” Step by Step

When you assign a task to an agent, hereâ€™s what happens behind the scenes:

1. **Reasoning**
   The agent uses the LLM as its *brain* to understand the task.

2. **Tool Selection**
   It checks which tools are available (like web search, Wikipedia, etc.) to accomplish the task.

3. **Action**
   It picks the most suitable tool and executes an action.

4. **Observation**
   The agent reviews the toolâ€™s output (it doesnâ€™t blindly trust it!).

5. **Re-evaluation**

   * If satisfied, it uses that information to produce the final response.
   * If not, it rethinks its approach â€” maybe tries another tool or rephrases the query.

6. **Completion**
   Once the result looks solid, it shares the **final answer** with the user.

> ðŸ§© This iterative cycle of â€œ**Reason â†’ Act â†’ Observe â†’ Reason again**â€ is what gives the ReAct Agent its name and intelligence.

---

## âš™ï¸ Agent Support in LLMs

Most modern LLMs â€” like GPT-4 or Claude â€” can support **agent-like behavior** when integrated with a framework such as **LangChain**.

> ðŸ“˜ **Tip:** To explore different agent types, search
> `langchain agent types` on Google or visit
> ðŸ‘‰ [LangChain Agent Types](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/)

---

## ðŸ§© Steps to Create an Agent

Letâ€™s build one from scratch using LangChainâ€™s **ReAct Agent** type.

### 1. Choose a Prompt

A prompt defines **how your agent thinks and behaves**.

* No need to write a prompt manually â€” LangChain provides reusable templates.
* For ReAct agents, we can pull a ready-made prompt from the **LangChain Hub**.

```python
from langchain import hub
# Get the prompt to use (you can customize this!)
prompt = hub.pull("hwchase17/react")
```

ðŸ“„ You can preview this prompt here:
ðŸ‘‰ [hwchase17/react](https://smith.langchain.com/hub/hwchase17/react)

Youâ€™ll notice placeholders like `{tools}` â€” theyâ€™re dynamically filled with the list of tools you load.

---

## ðŸ§° Setting Up the ReAct Agent (Code Example)

Letâ€™s build an AI Agent that can search the web and query Wikipedia.

### 1. Install dependencies

Make sure you have these installed:

* `wikipedia`
* `duckduckgo-search`

These give the agent access to public information sources.

---

### 2. Skeleton Code

```python
import os
from langchain_openai import ChatOpenAI
from langchain import hub
import streamlit as st
from langchain.agents import create_react_agent, AgentExecutor
from langchain_community.agent_toolkits.load_tools import load_tools

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
llm = ChatOpenAI(model="gpt-4o", api_key=OPENAI_API_KEY)

prompt = hub.pull("hwchase17/react")
```

---

### 3. Load Tools

```python
tools = load_tools(["wikipedia", "ddg-search"])
```

These tools allow the agent to:

* Use **Wikipedia** for structured knowledge
* Use **DuckDuckGo** for real-time internet search

---

### 4. Create the Agent

```python
agent = create_react_agent(llm, tools, prompt)
```

---

### 5. Create an Agent Executor

```python
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
```

> ðŸ§­ **Callout:**
> The `AgentExecutor` acts like the **manager** that runs the agentâ€™s reasoning-action loop until it reaches a conclusion.

---

### 6. Add a Streamlit UI

```python
st.title("This is an AI Agent")
task = st.text_input("Assign me a task")
if task:
    response = agent_executor.invoke({"input": task})
    st.write(response["output"])
```

---

### âœ… Final Code

```python
import os
from langchain_openai import ChatOpenAI
from langchain import hub
import streamlit as st
from langchain.agents import create_react_agent, AgentExecutor
from langchain_community.agent_toolkits.load_tools import load_tools
from numpy.f2py.crackfortran import verbose

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
llm = ChatOpenAI(model="gpt-4o", api_key=OPENAI_API_KEY)

prompt = hub.pull("hwchase17/react")

tools = load_tools(["wikipedia", "ddg-search"])
agent = create_react_agent(llm, tools, prompt)

agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

st.title("This is an AI agent")
task = st.text_input("Assign me a task")
if task:
    response = agent_executor.invoke({"input": task})
    st.write(response["output"])
```

Run it with:

```
streamlit run agent_demo.py
```

> âš ï¸ **Note:**
> In some corporate networks, `wikipedia` and `duckduckgo` may be blocked.
> If thatâ€™s the case, try running it in **Google Colab** instead.

---

## ðŸ§ª Testing Your Agent

Try these example prompts:

* **â€œPopulation of Japanâ€** â†’ uses *DuckDuckGo* search
* **â€œExplain the concept of Blockchainâ€** â†’ uses *Wikipedia*

> ðŸ’¬ The agent dynamically decides which tool fits best for each query.

---

## ðŸžï¸ Landmark Helper â€” A Creative Use Case

Imagine you upload an image of a landmark and ask the AI questions about it.
Hereâ€™s how we can make that happen.

### Approach

1. **Identify the landmark** using an image-processing model.
2. **Use the agent** (with Wikipedia and search tools) to answer questions about it.

ðŸ“˜ Check this working demo notebook:
[Agentive Landmark Helper](https://github.com/kaushik912/genai_code/blob/main/mynotebooks/agentive_landmark_helper.ipynb)

---

### Sample Code (Google Colab Example)

```python
# -*- coding: utf-8 -*-
"""Welcome To Colab"""

import base64, os, getpass
from langchain import hub
from langchain.agents import create_react_agent, AgentExecutor
from langchain_community.agent_toolkits.load_tools import load_tools
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("OPENAI_API_KEY")

def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
llm = ChatOpenAI(model="gpt-4o", api_key=OPENAI_API_KEY)
image = encode_image("/content/sample_data/indian.jpg")

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant that can identify a landmark."),
    ("human", [
        {"type": "text", "text": "Return the landmark name only. Do not explain."},
        {"type": "image_url", "image_url": {
            "url": f"data:image/jpeg;base64,{image}", "detail": "low"
        }},
    ]),
])

chain = prompt | llm

print("Landmark Helper")
question = input("Enter a question about the landmark: ")

task = None
if question:
    response = chain.invoke({"image": image})
    task = question + " " + response.content

prompt = hub.pull("hwchase17/react")
tools = load_tools(["wikipedia", "ddg-search"])
agent = create_react_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

if task:
    print("Task entered to agent =", task)
    response = agent_executor.invoke({"input": task})
    print(response["output"])
```

> ðŸ’¡ **Pro Tip:**
> The phrase `"Return the landmark name only. Do not explain."` ensures the LLM returns *just* the landmarkâ€™s name â€” clean and precise!

---

## ðŸ§© Open-Source Equivalent

If you prefer open-source or local options:

* Try using **`llava:7b`** through **Ollama** â€” it performs well for similar visual question-answering tasks.

---

## âœ¨ Key Takeaways

* **AI Agents** combine reasoning (LLMs) and action (tools).
* **ReAct agents** use a reasoning-action loop for intelligent decision-making.
* **LangChain** simplifies building these agents with reusable prompts and toolkits.
* With a few lines of code, you can build **interactive**, **autonomous**, and even **vision-capable** AI assistants.

