# ðŸš€ Getting Started with LangChain and Local LLMs

This guide will help you set up everything you need to start working with **LangChain** and **open-source LLMs** (via **Ollama**) â€” all within **PyCharm IDE**.

---

## ðŸ§© Step 1: Install Required Packages

To begin, open **PyCharm â†’ Preferences â†’ Python Packages**, and install the following:

```
langchain
langchain_openai
langchain_community
```

These are essential to interact with local or hosted LLMs using LangChain.

---

## ðŸ’» Step 2: Use PyCharm IDE

We'll use **PyCharm Community Edition** throughout these notes and exercises.
Itâ€™s great for Python-based AI workflows and integrates nicely with virtual environments.

---

## ðŸ¦™ Step 3: Install and Explore Ollama

Ollama is your **local LLM runtime** â€” it lets you download and run open-source models right on your machine.

### ðŸ§  Installation

1. [Install Ollama](https://ollama.ai) (simple installer for macOS, Windows, or Linux).

### âš¡ Try These Commands

After installation, open your terminal and experiment with:

| Command                | Description                                                                     |
| ---------------------- | ------------------------------------------------------------------------------- |
| `ollama list`          | Lists all available models                                                      |
| `ollama pull gemma:2b` | Downloads the *Gemma 2B* model (takes time depending on network and model size) |
| `ollama run gemma:2b`  | Runs the model â€” opens a prompt (`>>>`) to chat with it                         |
| `ollama ps`            | Shows running models                                                            |
| `ollama rm gemma:2b`   | Removes the Gemma 2B model                                                      |
| `ollama run mistral`   | Runs the *Mistral* model                                                        |
| `ollama run llama3`    | Runs the *Llama 3* model                                                        |

ðŸ’¬ **Example Interaction:**

```text
>>> what is the capital of India?
```

Type `/bye` or press `CTRL + D` to exit the Ollama prompt.

> ðŸ’¡ Tip: If Ollama is already running, you can access it in your browser at **[http://localhost:11434](http://localhost:11434)**

---

## ðŸ§¬ Step 4: Know Your Models â€” Quick Overview

| Model       | Developer       | Highlights                                                                                                                                                            |
| ----------- | --------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Llama 3** | Meta AI         | Open-source, multilingual (30+ languages), strong in English and coding. Available in 8B, 70B, and 405B variants. Ideal for building and customizing AI applications. |
| **Mistral** | Mistral AI      | Highly efficient open-source model optimized for local use â€” great performance even on limited hardware.                                                              |
| **Gemma 2** | Google DeepMind | Lightweight and fast. Excellent for code completion, natural language understanding, reasoning, and instruction following â€” perfect for local deployments.            |

---

## ðŸ§ª Step 5: Practice Exercises

* ðŸ“¥ **Download multiple models** (e.g., Llama, Mistral, Gemma) **in the office** â€” VPN connections can slow downloads later.
* ðŸ•¹ï¸ **Experiment** with each model:

  * Ask questions
  * Compare outputs
  * Observe how each model handles reasoning, coding, and conversation

> The more you play, the faster youâ€™ll understand their strengths and differences!

---

### âœ… Summary

You now have:

* LangChain + OpenAI integration ready
* Ollama installed for local LLMs
* Hands-on experience with **Llama 3**, **Mistral**, and **Gemma 2**

Next step: start connecting LangChain with Ollama models to build your own AI workflows!


