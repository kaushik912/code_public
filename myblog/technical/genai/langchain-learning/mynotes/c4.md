# ğŸš€ Getting Started with OpenAI and Local LLMs

A fun, hands-on guide to setting up and testing both **OpenAI-hosted** and **open-source local models** using **LangChain**.

---

## ğŸ§© Prerequisites (Do These First!)

Before writing your first AI program, make sure youâ€™ve completed these setup steps:

1. **Install PyCharm** â€” your IDE for writing Python.
2. **Get OpenAI Credits** â€” at least **$10 worth** (youâ€™ve got this covered âœ…).
3. **Generate an API Token** in your [OpenAI account](https://platform.openai.com/).
4. **Test your API Token** using the following `curl` command.

   ```bash
   curl --location --request POST 'https://api.openai.com/v1/chat/completions' \
   --header 'Content-Type: application/json' \
   --header 'Authorization: Bearer <secret_token>' \
   --data-raw '{
       "model": "gpt-4o-mini",
       "messages": [
         {
           "role": "system",
           "content": "You are a helpful assistant."
         },
         {
           "role": "user",
           "content": "Hello!"
         }
       ]
     }'
   ```

   âœ… **Expected Output:** HTTP `200 OK` â€” meaning your setup works perfectly!

---

## ğŸ’» First Program â€” Woohoo!

Letâ€™s write your **first Python program** that interacts with OpenAIâ€™s **GPT-4o** model.

### Step 1: Install Required Package

In your PyCharm terminal, install:

```bash
pip install langchain_openai
```

### Step 2: Set Up Your API Key

Export your OpenAI key in your shell configuration:

```bash
export OPENAI_API_KEY=<api_token>
```

Add this line in your `~/.zshrc` or `~/.bashrc` file.

### Step 3: Write the Code

Hereâ€™s your minimal working example:

```python
import os
from langchain_openai import ChatOpenAI

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
llm = ChatOpenAI(model="gpt-4o", api_key=OPENAI_API_KEY)

question = input("Enter the question: ")
response = llm.invoke(question)
print(response.content)
```

ğŸ§  **Try it out!**
Ask a question like:

> *â€œWhat is the capital of India?â€*

The model should reply: **New Delhi** âœ…

---

## ğŸŒ Open Source Models â€” Local LLM Fun

Next, letâ€™s run an **open-source model** locally using **Ollama** and LangChainâ€™s **community** integration.

### Step 1: Install Package

```bash
pip install langchain_community
```

### Step 2: Run the Gemma Model

Use the following script:

```python
import os
from langchain_community.chat_models import ChatOllama

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
llm = ChatOllama(model="gemma:2b")

question = input("Enter the question: ")
response = llm.invoke(question)
print(response.content)
```

> ğŸ’¡ **Note:** This uses the **Ollama server** running on `localhost:11434`.

This demonstrates **local LLM testing** â€” no cloud required!
You can also try **`llama3.2:latest`**, which often gives excellent results.

---

## ğŸ§  Exercise â€” Try Mistral!

Time to test your skills!
Letâ€™s swap in another model â€” **Mistral** â€” for the same program.

### Step 1: Pull the Model

Run this in your terminal:

```bash
ollama pull mistral
```

### Step 2: Modify the Code

Change just one line â€” the model name!

```python
import os
from langchain_community.chat_models import ChatOllama

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ğŸ‘‡ Only this line changes!
llm = ChatOllama(model="mistral")

question = input("Enter the question: ")
response = llm.invoke(question)
print(response.content)
```

ğŸ§© **Everything else remains the same.**
Test it by asking again:

> â€œWhat is the capital of India?â€

ğŸ‰ The model should respond with **New Delhi** â€” success!

---

## âš¡ Quick Recap

| Task          | Model      | Package               | Server             |
| ------------- | ---------- | --------------------- | ------------------ |
| OpenAI GPT-4o | `gpt-4o`   | `langchain_openai`    | OpenAI Cloud       |
| Open Source   | `gemma:2b` | `langchain_community` | Ollama (Localhost) |
| Exercise      | `mistral`  | `langchain_community` | Ollama (Localhost) |

