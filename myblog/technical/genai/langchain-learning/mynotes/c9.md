# ðŸš€ Understanding Sequential Chain Complexes in LangChain

## ðŸ“˜ Overview

In this session, weâ€™ll explore **sequential chains** â€” a powerful LangChain concept that lets you connect multiple LLM calls into a single logical flow.

Weâ€™ll walk through practical examples:

1. **Speech Generator** â€” generates a title and then a full speech.
2. **Blog Post Generator** â€” builds an outline and then a paragraph.
3. **Using Multiple LLMs** â€” how to mix different models in one chain.

---

## ðŸ§© Example 1: Sequential Chain â€” *Speech Generator*

### ðŸ’¡ Goal

We want to:

1. Generate a **title** for a given topic.
2. Use that title to generate a **350-word speech**.

---

### ðŸ§  Code Walkthrough

```python
import os
from langchain_openai import ChatOpenAI
import streamlit as st
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
llm = ChatOpenAI(model="gpt-4o", api_key=OPENAI_API_KEY)

title_prompt = PromptTemplate(
    input_variables=["topic"],
    template="""You are an experienced speech writer.
    You need to craft an impactful title for a speech 
    on the following topic: {topic}
    Answer exactly with one title."""
)

speech_prompt = PromptTemplate(
    input_variables=["title"],
    template="""You need to write a powerful speech of 350 words
     for the following title: {title}"""
)

first_chain = title_prompt | llm | StrOutputParser() | (lambda title: (st.write(title), title)[1])
second_chain = speech_prompt | llm
final_chain = first_chain | second_chain

st.title("Speech Generator")

topic = st.text_input("Enter the topic:")

if topic:
    response = final_chain.invoke({"topic": topic})
    st.write(response.content)
```

---

### ðŸ” Key Concepts

#### ðŸ§± 1. Response Parsing

```python
llm | StrOutputParser() | (lambda title: (st.write(title), title)[1])
```

This section does **three things**:

* âœ… **Parses the LLMâ€™s output** into a clean string using `StrOutputParser()`.
* ðŸ–¥ï¸ **Displays** the title in Streamlit with `st.write()`.
* ðŸ” **Returns** the `title` value (via tuple indexing `[1]`) so it can be passed to the next chain.

---

#### ðŸ”— 2. Understanding the Chaining

You are essentially connecting each step like a pipeline:

| Chain Name     | Function               | Output       |
| -------------- | ---------------------- | ------------ |
| `first_chain`  | Generates a **title**  | `title`      |
| `second_chain` | Generates a **speech** | `speech`     |
| `final_chain`  | Connects both          | Final speech |

**Visual Concept:**

```
topic â†’ [first_chain â†’ title] â†’ [second_chain â†’ speech]
```

Thatâ€™s **LCEL** (LangChain Expression Language) in action!

---

## ðŸ“˜ Quick Refresher: Tuples (Optional)

If the `lambda` expression confused you, hereâ€™s a quick recap.

```python
my_tuple = (1, "hello", 3.14, True)
print(my_tuple[1])
```

ðŸ–¨ï¸ Output:

```
hello
```

> âœ… Tuples are **ordered**, **immutable** collections that can hold multiple data types.

---

## âœï¸ Exercise: Blog Post Generator

Letâ€™s apply what we learned to a **Blog Post Generator**.

### ðŸŽ¯ Task

1. Generate a **blog outline** from a topic.
2. Generate an **engaging introduction paragraph** from the outline.

---

### ðŸ’» Sample Code

```python
import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
import os

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
llm = ChatOpenAI(model="gpt-4", api_key=OPENAI_API_KEY)

outline_prompt = PromptTemplate(
    input_variables=["topic"],
    template="""You are a professional blogger.
    Create an outline for a blog post on the following topic: {topic}
    The outline should include:
    - Introduction
    - 3 main points with subpoints
    - Conclusion"""
)

introduction_prompt = PromptTemplate(
    input_variables=["outline"],
    template="""You are a professional blogger.
    Write an engaging introduction paragraph based on the following
    outline: {outline}
    The introduction should hook the reader and provide a brief
    overview of the topic."""
)

first_chain = outline_prompt | llm | StrOutputParser() | (lambda outline: (st.write(outline), outline)[1])
second_chain = introduction_prompt | llm
overall_chain = first_chain | second_chain

st.title("Blog Post Generator")

topic = st.text_input("Input Topic")

if topic:
    response = overall_chain.invoke({"topic": topic})
    st.write(response.content)
```

---

### ðŸ§© Chaining Logic

| Chain           | Description                                        |
| --------------- | -------------------------------------------------- |
| `first_chain`   | Generates the **outline**                          |
| `second_chain`  | Uses the outline to create the **intro paragraph** |
| `overall_chain` | Combines both sequentially                         |

**Concept Flow:**

```
topic â†’ [first_chain â†’ outline] â†’ [second_chain â†’ intro paragraph]
```

This mirrors the same structure as the Speech Generator example.

---

## âš™ï¸ Advanced Example: Using Multiple LLMs

Sometimes, you might want to use **different models** for different stages â€” for example, GPT for creativity and Mistral for refinement.

### ðŸ’» Example Code

```python
import os
from langchain_openai import ChatOpenAI
import streamlit as st
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.chat_models import ChatOllama

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

llm1 = ChatOpenAI(model="gpt-4o", api_key=OPENAI_API_KEY)
llm2 = ChatOllama(model="mistral")

title_prompt = PromptTemplate(
    input_variables=["topic"],
    template="""You are an experienced speech writer.
    You need to craft an impactful title for a speech 
    on the following topic: {topic}
    Answer exactly with one title."""
)

speech_prompt = PromptTemplate(
    input_variables=["title"],
    template="""You need to write a powerful speech of 350 words
     for the following title: {title}"""
)

first_chain = title_prompt | llm1 | StrOutputParser() | (lambda title: (st.write(title), title)[1])
second_chain = speech_prompt | llm2
final_chain = first_chain | second_chain

st.title("Speech Generator")

topic = st.text_input("Enter the topic:")

if topic:
    response = final_chain.invoke({"topic": topic})
    st.write(response.content)
```

---

### ðŸ”— Chaining with Multiple Models

| Chain          | Model Used    | Description                      |
| -------------- | ------------- | -------------------------------- |
| `first_chain`  | GPT-4o        | Generates the **speech title**   |
| `second_chain` | Mistral       | Generates the **speech content** |
| `final_chain`  | Combines both | End-to-end workflow              |

**Flow:**

```
topic â†’ [GPT-4o â†’ title] â†’ [Mistral â†’ speech]
```

---

## ðŸ§  Key Takeaways

> ðŸ’¬ **Sequential Chaining** lets you build powerful, multi-step workflows using simple logic.

### ðŸ”‘ Remember:

* Each chain produces **output** that becomes **input** for the next.
* Use `StrOutputParser()` to cleanly extract text responses.
* `lambda` functions can display and return values in one step.
* You can mix and match **different LLMs** for specialized tasks.

---

âœ… **Next Steps:**
Try creating your own chained applications!
For example:

* Idea â†’ Slogan â†’ Ad Copy
* Topic â†’ Quiz Questions â†’ Explanations
* Headline â†’ Summary â†’ Call-to-Action

